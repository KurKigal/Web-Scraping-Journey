# âš–ï¸ Web Scraping EtiÄŸi ve Yasal Konular

## ğŸ¯ Neden Etik Ã–nemli?

Web scraping gÃ¼Ã§lÃ¼ bir araÃ§ ama **bÃ¼yÃ¼k gÃ¼Ã§, bÃ¼yÃ¼k sorumluluk getirir**. YanlÄ±ÅŸ kullanÄ±m:
- Web sitelerini Ã§Ã¶kertebilir
- Yasal sorunlara yol aÃ§abilir  
- Ä°ÅŸ kaybÄ±na sebep olabilir
- Etik olmayan veri kullanÄ±mÄ±na neden olabilir

## ğŸ“œ Temel Etik Ä°lkeler

### 1. **"Ä°yi KomÅŸu" Prensibi**
```
Soru: Bu web sitesinin sahibi olsaydÄ±m, 
      bÃ¶yle scraping yapÄ±lmasÄ±na razÄ± olur muydum?

âœ… Makul hÄ±zda, makul sayÄ±da istek
âŒ Siteyi Ã§Ã¶kertirecek kadar yoÄŸun istek
```

### 2. **ÅeffaflÄ±k Prensibi**
```
âœ… User-Agent'Ä±nÄ±zda kim olduÄŸunuzu belirtin
âœ… Ä°letiÅŸim bilgilerinizi paylaÅŸÄ±n
âŒ Bot olduÄŸunuzu gizlemeye Ã§alÄ±ÅŸmayÄ±n
```

### 3. **Zarar Vermeme Prensibi**
```
âœ… Rate limiting kullanÄ±n (istekler arasÄ± bekleme)
âœ… Hata durumunda denemeleri durdurun
âŒ AynÄ± anda yÃ¼zlerce istek gÃ¶ndermeyin
```

## ğŸ¤– robots.txt - Sitenin KurallarÄ±

### robots.txt Nedir?
Web sitelerinin botlara yÃ¶nelik kurallarÄ±nÄ± yazdÄ±ÄŸÄ± dosya:
```
https://example.com/robots.txt
```

### Ã–rnek robots.txt
```
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/
Crawl-delay: 5

User-agent: Googlebot
Allow: /

User-agent: BadBot
Disallow: /
```

### Python'da robots.txt KontrolÃ¼
```
Ä°leriki bÃ¶lÃ¼mlerde robots.txt dosyasÄ±nÄ± otomatik olarak 
kontrol eden kod Ã¶rneklerini Ã¶ÄŸreneceÄŸiz.

Åimdilik manuel olarak kontrol edin:
1. TarayÄ±cÄ±nÄ±zda: https://example.com/robots.txt
2. Scraping yapmak istediÄŸiniz URL'yi kontrol edin
3. "Disallow" listesinde var mÄ± bakÄ±n
```

## âš¡ Rate Limiting - HÄ±z SÄ±nÄ±rlamasÄ±

### Neden Gerekli?
```
Analoji: Market alÄ±ÅŸveriÅŸi
- Normal hÄ±z: Her Ã¼rÃ¼nÃ¼ bakÄ±p alma (Normal kullanÄ±cÄ±)
- Ã‡ok hÄ±zlÄ±: KoÅŸarak alÄ±ÅŸveriÅŸ yapma (Bot gibi)
- AÅŸÄ±rÄ± hÄ±z: Marketi istila etme (DDoS saldÄ±rÄ±sÄ±)
```

### Rate Limiting Nedir?
```
Rate limiting = Ä°stekler arasÄ± bekleme sÃ¼resi

Ã–rnekler:
- Her istek arasÄ±nda 1-2 saniye beklemek
- Dakikada maksimum 10 istek gÃ¶ndermek
- Server yoÄŸunken daha uzun beklemek

Bu konuyu 02-bs4-requests bÃ¶lÃ¼mÃ¼nde pratik olarak Ã¶ÄŸreneceÄŸiz.
```

## ğŸ“‹ Yasal Durumlar

### âœ… Genellikle Yasal Olan Durumlar
- **Halka aÃ§Ä±k veriler** (haber siteleri, Ã¼rÃ¼n fiyatlarÄ±)
- **Kendi sitenizi** scraping yapmak
- **API alternatifi olmayan** durumlar
- **Akademik araÅŸtÄ±rma** amaÃ§lÄ± kullanÄ±m
- **Fiyat karÅŸÄ±laÅŸtÄ±rma** siteleri

### âš ï¸ Dikkatli OlunmasÄ± Gereken Durumlar  
- **KiÅŸisel veriler** (email, telefon, adres)
- **Telif hakkÄ± korumalÄ± iÃ§erik** (resimler, yazÄ±lar)
- **Login gerektiren alanlar**
- **Ticari kullanÄ±m** 

### âŒ Kesinlikle Yasak Olan Durumlar
- **KVKV ihlali** (kiÅŸisel verileri izinsiz kullanma)
- **Sitenin ToS'unu ihlal** (Terms of Service)
- **DDoS benzeri saldÄ±rÄ±lar**
- **Telif hakkÄ± ihlali**

## ğŸ”’ KiÅŸisel Veri KorumasÄ± (KVKV/GDPR)

### KVKV Uyumlu Scraping Ä°lkeleri
```
1. Veri Minimizasyonu
   - Sadece ihtiyacÄ±nÄ±z olan veriyi toplayÄ±n
   - Gereksiz kiÅŸisel bilgi almayÄ±n

2. AnonimleÅŸtirme  
   - KiÅŸisel verileri kimlik belirleyici olmayacak ÅŸekilde iÅŸleyin
   - Hash fonksiyonlarÄ± kullanÄ±n

3. Saklama SÃ¼resi
   - Veriyi sÃ¼resiz saklamayÄ±n
   - KullanÄ±m amacÄ± bitince silin

Teknik detaylarÄ± ilerki bÃ¶lÃ¼mlerde Ã¶ÄŸreneceÄŸiz.
```

## ğŸ›¡ï¸ Etik Scraping Checklist

Scraping yapmadan Ã¶nce ÅŸu sorularÄ± kendinize sorun:

### âœ… Teknik Kontroller
- [ ] robots.txt dosyasÄ±nÄ± kontrol ettim
- [ ] Rate limiting kullanÄ±yorum  
- [ ] User-Agent belirttim
- [ ] Hata durumlarÄ±nÄ± yÃ¶netiyorum
- [ ] Sunucuya zarar vermeyecek ÅŸekilde Ã§alÄ±ÅŸÄ±yor

### âœ… Yasal Kontroller
- [ ] Halka aÃ§Ä±k verilerle Ã§alÄ±ÅŸÄ±yorum
- [ ] KiÅŸisel veri toplamÄ±yorum
- [ ] Telif hakkÄ± ihlali yapmÄ±yorum
- [ ] Terms of Service'i okudum
- [ ] Ticari kullanÄ±m izinlerim var

### âœ… Etik Kontroller
- [ ] Site sahibi yerinde olsam bu scraping'e razÄ± olur muydum?
- [ ] Veriyi sorumlu ÅŸekilde kullanacaÄŸÄ±m
- [ ] Kaynak web sitesine atÄ±f yapacaÄŸÄ±m
- [ ] Gerekirse site sahibiyle iletiÅŸime geÃ§ebilirim

## ğŸ’¡ Ä°yi Pratikler

### 1. User-Agent Belirtme
```
User-Agent: Web tarayÄ±cÄ±nÄ±zÄ±n kimlik bilgisi
Ã–rnekler: Chrome, Firefox, Safari, veya Ã¶zel bot adÄ±

Ä°yi Ã¶rnek: "Educational Bot; contact: email@example.com"
KÃ¶tÃ¼ Ã¶rnek: GerÃ§ek tarayÄ±cÄ± gibi gÃ¶rÃ¼nmeye Ã§alÄ±ÅŸmak
```

### 2. Cache KullanÄ±mÄ±  
```
Cache = Daha Ã¶nce indirilen sayfalarÄ± tekrar kullanma

AvantajlarÄ±:
- AynÄ± sayfayÄ± tekrar indirmez
- Server'a daha az yÃ¼k
- Daha hÄ±zlÄ± Ã§alÄ±ÅŸma

Teknik detaylarÄ± ileriki bÃ¶lÃ¼mlerde.
```

### 3. Hata YÃ¶netimi
```
Nelere dikkat edin:
- Sayfa bulunamadÄ± (404)
- Server hatasÄ± (500) 
- Rate limit aÅŸÄ±mÄ± (429)
- BaÄŸlantÄ± problemleri

Her durumda nasÄ±l davranacaÄŸÄ±nÄ±zÄ± planlayÄ±n.
```

## ğŸ¯ SonuÃ§

Web scraping yaparken:
1. **Teknik yetkinlik** kadar **etik bilinci** de geliÅŸtirin
2. **"ZararsÄ±z kullanÄ±m"** prensibini benimseyin  
3. **Yasal sÄ±nÄ±rlarÄ±** Ã¶ÄŸrenin ve uygulayÄ±n
4. **Ä°yi komÅŸuluk** iliÅŸkileri kurun

**UnutmayÄ±n:** Etik scraping, sÃ¼rdÃ¼rÃ¼lebilir scraping demektir. BugÃ¼n kurallara uyarsanÄ±z, yarÄ±n da scraping yapabilirsiniz.

---

**Sonraki adÄ±m:** `02-bs4-requests/basic_scrape.py` - Ä°lk Ã¶rneÄŸimiz!

